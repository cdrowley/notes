{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install nltk\n",
    "# ! pip install textblob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "\n",
    "# # In case any corpus are missing \n",
    "# nltk.download()\n",
    "\n",
    "def text_preproc(x):\n",
    "    x = x.lower()\n",
    "    x = ' '.join([word for word in x.split(' ') if word not in stop_words])\n",
    "    x = unidecode(x).encode('ascii', 'ignore').decode()\n",
    "\n",
    "    x = re.sub(r'https*\\S+', ' ', x)\n",
    "    x = re.sub(r'@\\S+', ' ', x)\n",
    "    x = re.sub(r'#\\S+', ' ', x)\n",
    "    x = re.sub(r'\\'\\w+', '', x)\n",
    "    x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
    "    x = re.sub(r'\\w*\\d+\\w*', '', x)\n",
    "    x = re.sub(r'\\s{2,}', ' ', x)\n",
    "    return x\n",
    "\n",
    "\n",
    "#df = pd.read_csv('')\n",
    "#stop_words = stopwords.words(\"english\")\n",
    "#wordnet = WordNetLemmatizer() # what's this for?\n",
    "\n",
    "#df['clean_text'] = df['text'].apply(text_preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch this airport get swallowed up by a sandstorm in under a minute http://t.co/tvyqczgjdy'"
      ]
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example\n",
    "x = \"Watch This Airport Get Swallowed Up By A Sandstorm In Under A Minute http://t.co/TvYQczGJdy\"\n",
    "x = x.lower() # upper/lower/title\n",
    "display(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reddit Will Now Quarantine\\x89Û_ http://t.co/pkUAMXw6pm #onlinecommunities #reddit #amageddon #freespeech #Business http://t.co/PAWvNJ4sAP'"
      ]
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'reddit will now quarantine_ http://t.co/pkuamxw6pm #onlinecommunities #reddit #amageddon #freespeech #business http://t.co/pawvnj4sap'"
      ]
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Stale\n"
     ]
    }
   ],
   "source": [
    "# To just remove none ascii characters\n",
    "x = \"Reddit Will Now QuarantineÛ_ http://t.co/pkUAMXw6pm #onlinecommunities #reddit #amageddon #freespeech #Business http://t.co/PAWvNJ4sAP\"\n",
    "display(x)\n",
    "\n",
    "# Remove unicode characters\n",
    "x = x.encode('ascii', 'ignore').decode().lower()\n",
    "display(x)\n",
    "\n",
    "# To replace with ascii equivalent\n",
    "# !pip install unidecode\n",
    "from unidecode import unidecode\n",
    "\n",
    "txt = \"My name is Ståle\"\n",
    "txt = unidecode(txt)\n",
    "\n",
    "print(txt)\n",
    "\n",
    "# import unicodedata as ud\n",
    "\n",
    "# def get_ascii_char(c):\n",
    "#     s = ud.decomposition(c)\n",
    "#     if s == '': # for an indecomposable character, it returns ''\n",
    "#         return c\n",
    "#     code = int('0x' + s.split()[0], 0)\n",
    "#     return chr(code)\n",
    "\n",
    "# x = 'My name is Ståle'\n",
    "# y = ''\n",
    "# for i in x:\n",
    "#     y += get_ascii_char(i)\n",
    "    \n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "America like South Africa traumatised sick country - different ways course - still messed up.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords') #nltk.download() # just download all-nltk\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Example\n",
    "x = \"America like South Africa is a traumatised sick country - in different ways of course - but still messed up.\"\n",
    "\n",
    "# Remove stop words:\n",
    "x = ' '.join([word for word in x.split(' ') if word not in stop_words])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"     and   can't survive without referring   . Without Mr Modi they are BIG ZEROS\""
      ]
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Are people not concerned that after   obliteration in Scotland   UK is ripping itself apart over   contest?'"
      ]
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Notley tactful yet very direct response to Harper attack on Alberta gov. Hell YEAH Premier! http://t.co/rzSUlzMOkX #ableg #cdnpoli'"
      ]
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'In 2014 I will only smoke crqck if I becyme a mayor This includes Foursquare'"
      ]
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'C- specially modified to land in a stadium and rescue hostages in Iran in ... http://t.co/ http://t.co/'"
      ]
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\" and can't survive without referring . Without Mr Modi they are BIG ZEROS\""
      ]
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove terms like mentions, hashtags, links, and more.\n",
    "# https://lzone.de/examples/Python%20re.sub\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Remove mentions\n",
    "x = \"@DDNewsLive @NitishKumar  and @ArvindKejriwal can't survive without referring @@narendramodi . Without Mr Modi they are BIG ZEROS\"\n",
    "x = re.sub(\"@\\S+\", \" \", x)\n",
    "display(x)\n",
    "\n",
    "# Remove Hashtags\n",
    "x = \"Are people not concerned that after #SLAB's obliteration in Scotland #Labour UK is ripping itself apart over #Labourleadership contest?\"\n",
    "x = re.sub(\"#\\S+\", \" \", x)\n",
    "display(x)\n",
    "\n",
    "# Remove ticks and the next character\n",
    "x = \"Notley's tactful yet very direct response to Harper's attack on Alberta's gov't. Hell YEAH Premier! http://t.co/rzSUlzMOkX #ableg #cdnpoli\"\n",
    "x = re.sub(\"\\'\\w+\", '', x)\n",
    "display(x)\n",
    "\n",
    "# Remove punctuation\n",
    "x = \"In 2014 I will only smoke crqck if I becyme a mayor. This includes Foursquare.\"\n",
    "x = re.sub('[%s]' % re.escape(string.punctuation), '', x)\n",
    "display(x)\n",
    "\n",
    "# Remove numbers \n",
    "x = \"C-130 specially modified to land in a stadium and rescue hostages in Iran in 1980... http://t.co/tNI92fea3u http://t.co/czBaMzq3gL\"\n",
    "x = re.sub('\\w*\\d+\\w*', '', x)\n",
    "display(x)\n",
    "\n",
    "# Replace multiple spaces\n",
    "x = \"     and   can't survive without referring   . Without Mr Modi they are BIG ZEROS\"\n",
    "x = re.sub('\\s{2,}', \" \", x)\n",
    "display(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
